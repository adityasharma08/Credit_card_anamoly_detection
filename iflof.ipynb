{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":23498,"sourceType":"datasetVersion","datasetId":310}],"dockerImageVersionId":28755,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### Data preprocessing\n* Exploratory Data Analysis\n* Features transformation\n* Features selection\n\n#### Algorithm used\n* Isolation Forest\n* Local Outlier Factor","metadata":{}},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom sklearn.manifold import TSNE\n\n#for data preprocessing\nfrom sklearn.decomposition import PCA\n\n#for modeling\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.ensemble import IsolationForest\n\n#filter warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-07T13:35:37.706216Z","iopub.execute_input":"2024-07-07T13:35:37.706559Z","iopub.status.idle":"2024-07-07T13:35:38.76205Z","shell.execute_reply.started":"2024-07-07T13:35:37.706497Z","shell.execute_reply":"2024-07-07T13:35:38.761089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/creditcard.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-07T13:35:38.764215Z","iopub.execute_input":"2024-07-07T13:35:38.764548Z","iopub.status.idle":"2024-07-07T13:35:41.115696Z","shell.execute_reply.started":"2024-07-07T13:35:38.764487Z","shell.execute_reply":"2024-07-07T13:35:41.11491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(df.Class)\nplt.show()\nprint(df.Class.value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-07-07T13:35:41.117253Z","iopub.execute_input":"2024-07-07T13:35:41.117499Z","iopub.status.idle":"2024-07-07T13:35:41.254474Z","shell.execute_reply.started":"2024-07-07T13:35:41.11745Z","shell.execute_reply":"2024-07-07T13:35:41.253659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"***Plotting the Data***\n\nUsing a technique called T-SNE, we can reduce the dimensions of the data and create a 2D plot. The objective here is to show that distance based anomaly detection methods might not work as well as other techniques on this dataset. This is because the positive cases are not too far away from the normal cases.","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot\ndf_plt=df[df['Class']==0].sample(1000)\ndf_plt_pos=df[df['Class']==1].sample(20)\ndf_plt=pd.concat([df_plt,df_plt_pos])\ny_plt=df_plt['Class']\nX_plt=df_plt.drop('Class',1)\nX_embedded = TSNE(n_components=2).fit_transform(X_plt)\npyplot.figure(figsize=(12,8))\npyplot.scatter(X_embedded[:,0], X_embedded[:,1], c=y_plt, cmap=pyplot.cm.get_cmap(\"Paired\", 2))\npyplot.colorbar(ticks=range(2))","metadata":{"execution":{"iopub.status.busy":"2024-07-07T13:35:41.256152Z","iopub.execute_input":"2024-07-07T13:35:41.25657Z","iopub.status.idle":"2024-07-07T13:35:46.492617Z","shell.execute_reply.started":"2024-07-07T13:35:41.256424Z","shell.execute_reply":"2024-07-07T13:35:46.491762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 0.17% fraud cases in the dataset which are anomalies which are shown by red dot and normal transaction i.e. not fraudulent cases are shown by blue dot","metadata":{}},{"cell_type":"code","source":"timedelta = pd.to_timedelta(df['Time'], unit='s')\ndf['Time_hour'] = (timedelta.dt.components.hours).astype(int)\n\nplt.figure(figsize=(12,5))\nsns.distplot(df[df['Class'] == 0][\"Time_hour\"], color='g')\nsns.distplot(df[df['Class'] == 1][\"Time_hour\"], color='r')\nplt.title('Fraud and Normal Transactions by Hours', fontsize=17)\nplt.xlim([-1,25])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-07T13:35:46.49541Z","iopub.execute_input":"2024-07-07T13:35:46.495728Z","iopub.status.idle":"2024-07-07T13:35:53.770727Z","shell.execute_reply.started":"2024-07-07T13:35:46.495669Z","shell.execute_reply":"2024-07-07T13:35:53.76979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seems like hour of day have some impact on number or fraud cases. Lets move to transform the remaining features.","metadata":{}},{"cell_type":"markdown","source":"### Feature Transformation\n\nLets transform the remaining features using PCA.","metadata":{}},{"cell_type":"code","source":"cols= df[['Time', 'Amount']]\n\npca = PCA()\npca.fit(cols)\nX_PCA = pca.transform(cols)\n\ndf['V29']=X_PCA[:,0]\ndf['V30']=X_PCA[:,1]\n\ndf.drop(['Time','Time_hour', 'Amount'], axis=1, inplace=True)\n\ndf.columns","metadata":{"execution":{"iopub.status.busy":"2024-07-07T13:35:53.772296Z","iopub.execute_input":"2024-07-07T13:35:53.772584Z","iopub.status.idle":"2024-07-07T13:35:53.911126Z","shell.execute_reply.started":"2024-07-07T13:35:53.772529Z","shell.execute_reply":"2024-07-07T13:35:53.910397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets have a view at distribution of features","metadata":{}},{"cell_type":"code","source":"columns = df.drop('Class', axis=1).columns\ngrid = gridspec.GridSpec(6, 5)\n\nplt.figure(figsize=(20,10*2))\n\nfor n, col in enumerate(df[columns]):\n    ax = plt.subplot(grid[n])\n    sns.distplot(df[df.Class==1][col], bins = 50, color='g')\n    sns.distplot(df[df.Class==0][col], bins = 50, color='r') \n    ax.set_ylabel('Density')\n    ax.set_title(str(col))\n    ax.set_xlabel('')\n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-07T13:47:36.146221Z","iopub.execute_input":"2024-07-07T13:47:36.146513Z","iopub.status.idle":"2024-07-07T13:47:47.102487Z","shell.execute_reply.started":"2024-07-07T13:47:36.146471Z","shell.execute_reply":"2024-07-07T13:47:47.101675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Selection using Z-test\n\nLets move to do some hypothesis testing to find statistically significant features. We will be performing `Z-test` with valid transactions as our population. \n\nSo the case is we have to find if the values of fraud transactions are significantly different from normal transaction or not for all features. The level of significance is 0.01 and its a two tailed test.\n\n#### Scenario:\n* Valid transactions as our population\n* Fraud transactions as sample\n* Two tailed Z-test\n* Level of significance 0.01\n* Corresponding critical value is 2.58\n\n#### Hypothesis:\n* H0: There is no difference (insignificant)\n* H1: There is a difference  (significant)\n\n#### Formula for z-score:\n\n$$ Zscore = (\\bar{x} - \\mu) / S.E$$","metadata":{}},{"cell_type":"code","source":"def ztest(feature):\n    \n    mean = normal[feature].mean()\n    std = fraud[feature].std()\n    zScore = (fraud[feature].mean() - mean) / (std/np.sqrt(sample_size))\n    \n    return zScore","metadata":{"execution":{"iopub.status.busy":"2024-07-07T13:36:08.292548Z","iopub.execute_input":"2024-07-07T13:36:08.292904Z","iopub.status.idle":"2024-07-07T13:36:08.299494Z","shell.execute_reply.started":"2024-07-07T13:36:08.292841Z","shell.execute_reply":"2024-07-07T13:36:08.298602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns= df.drop('Class', axis=1).columns\nnormal= df[df.Class==0]\nfraud= df[df.Class==1]\nsample_size=len(fraud)\nsignificant_features=[]\ncritical_value=2.58\n\nfor i in columns:\n    \n    z_vavlue=ztest(i)\n    \n    if( abs(z_vavlue) >= critical_value):    \n        print(i,\" is statistically significant\") #Reject Null hypothesis. i.e. H0\n        significant_features.append(i)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T13:36:08.300829Z","iopub.execute_input":"2024-07-07T13:36:08.301133Z","iopub.status.idle":"2024-07-07T13:36:08.442952Z","shell.execute_reply.started":"2024-07-07T13:36:08.301077Z","shell.execute_reply":"2024-07-07T13:36:08.442189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we have already seen from distribution plots that distribution of normal and fraud data of V13, V15, V22, V23, V25 and 26 features is almost same, now, its proven through hypothesis testing. We will eliminate these features from our dataset as they don't contribute at all.","metadata":{}},{"cell_type":"markdown","source":"#### Split data into Inliers and Outliers\n\n`Inliers` are values that are normal.`Outliers` are values that don't belong to normal data and they are the anomalies.","metadata":{}},{"cell_type":"code","source":"significant_features.append('Class')\ndf= df[significant_features]\n\ninliers = df[df.Class==0]\nins = inliers.drop(['Class'], axis=1)\n\noutliers = df[df.Class==1]\nouts = outliers.drop(['Class'], axis=1)\n\nins.shape, outs.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-07T13:36:08.444269Z","iopub.execute_input":"2024-07-07T13:36:08.44456Z","iopub.status.idle":"2024-07-07T13:36:08.577245Z","shell.execute_reply.started":"2024-07-07T13:36:08.444506Z","shell.execute_reply":"2024-07-07T13:36:08.576395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling\n.\n\n####  1. ISOLATION FOREST\n\nIsolation Forest is an unsupervised anomaly detection algorithm that uses the two properties “Few” and “Different” of anomalies to detect their existence. Since anomalies are few and different, they are more susceptible to isolation. This algorithm isolates each point in the data and splits them into outliers or inliers. This split depends on how long it takes to separate the points. If we try to separate a point which is obviously a non-outlier, it’ll have many points in its round, so that it will be really difficult to isolate. On the other hand, if the point is an outlier, it’ll be alone and we’ll find it very easily.\n\n####  2. Local Outlier Factor\n\nThe Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It is a calculation that looks at the neighbors of a certain point to find out its density and compare this to the density of neighbour points later on. In short we can say that the density around an outlier object is significantly different from the density around its neighbors. LOF considers as outliers the samples that have a substantially lower density than their neighbors.","metadata":{}},{"cell_type":"code","source":"def normal_accuracy(values):\n    \n    tp=list(values).count(1)\n    total=values.shape[0]\n    accuracy=np.round(tp/total,4)\n    \n    return accuracy\n\ndef fraud_accuracy(values):\n    \n    tn=list(values).count(-1)\n    total=values.shape[0]\n    accuracy=np.round(tn/total,4)\n    \n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2024-07-07T13:36:08.579Z","iopub.execute_input":"2024-07-07T13:36:08.579349Z","iopub.status.idle":"2024-07-07T13:36:08.585815Z","shell.execute_reply.started":"2024-07-07T13:36:08.579287Z","shell.execute_reply":"2024-07-07T13:36:08.585033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Isolation Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import IsolationForest\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Define accuracy functions (if not already defined)\ndef normal_accuracy(predictions):\n    return np.mean(predictions == 1)\n\ndef fraud_accuracy(predictions):\n    return np.mean(predictions == -1)\n\n# Set the random state for reproducibility\nstate = 42\n\n# Fit the Isolation Forest model\nISF = IsolationForest(random_state=state)\nISF.fit(ins)\n\n# Predict normal and fraud cases\nnormal_isf = ISF.predict(ins)\nfraud_isf = ISF.predict(outs)\n\n# Calculate accuracies\nin_accuracy_isf = normal_accuracy(normal_isf)\nout_accuracy_isf = fraud_accuracy(fraud_isf)\n\n# Print accuracies\nprint(\"Accuracy in Detecting Normal Cases:\", in_accuracy_isf)\nprint(\"Accuracy in Detecting Fraud Cases:\", out_accuracy_isf)\n\n# Calculate confusion matrix\nconf_matrix = confusion_matrix(np.concatenate((np.ones(len(ins)), -np.ones(len(outs)))), np.concatenate((normal_isf, fraud_isf)))\n\n# Calculate precision and recall\nprecision = precision_score(np.concatenate((np.ones(len(ins)), -np.ones(len(outs)))), np.concatenate((normal_isf, fraud_isf)), pos_label=-1)\nrecall = recall_score(np.concatenate((np.ones(len(ins)), -np.ones(len(outs)))), np.concatenate((normal_isf, fraud_isf)), pos_label=-1)\n\n# Print confusion matrix, precision, and recall\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Normal', 'Fraud'], yticklabels=['Normal', 'Fraud'])\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-07T13:40:45.951333Z","iopub.execute_input":"2024-07-07T13:40:45.951728Z","iopub.status.idle":"2024-07-07T13:41:06.512766Z","shell.execute_reply.started":"2024-07-07T13:40:45.951669Z","shell.execute_reply":"2024-07-07T13:41:06.512034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Local Outlier Factor","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n# Define accuracy functions (if not already defined)\ndef normal_accuracy(predictions):\n    return np.mean(predictions == 1)\n\ndef fraud_accuracy(predictions):\n    return np.mean(predictions == -1)\n\n# Fit the Local Outlier Factor model with novelty detection enabled\nLOF = LocalOutlierFactor(novelty=True)\nLOF.fit(ins)\n\n# Predict normal and fraud cases\nnormal_lof = LOF.predict(ins)\nfraud_lof = LOF.predict(outs)\n\n# Calculate accuracies\nin_accuracy_lof = normal_accuracy(normal_lof)\nout_accuracy_lof = fraud_accuracy(fraud_lof)\n\n# Print accuracies\nprint(\"Accuracy in Detecting Normal Cases:\", in_accuracy_lof)\nprint(\"Accuracy in Detecting Fraud Cases:\", out_accuracy_lof)\n\nfrom sklearn.metrics import confusion_matrix\n# Calculate confusion matrix\nconf_matrix = confusion_matrix(np.concatenate((np.ones(len(ins)), -np.ones(len(outs)))), np.concatenate((normal_lof, fraud_lof)))\n\n# Print confusion matrix\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Normal', 'Fraud'], yticklabels=['Normal', 'Fraud'])\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-07T13:42:55.097959Z","iopub.execute_input":"2024-07-07T13:42:55.098268Z","iopub.status.idle":"2024-07-07T13:45:54.89634Z","shell.execute_reply.started":"2024-07-07T13:42:55.098226Z","shell.execute_reply":"2024-07-07T13:45:54.895643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1,ax2)= plt.subplots(1,2, figsize=[10,2])\n\nax1.set_title(\"Accuracy of Isolation Forest\",fontsize=10)\nsns.barplot(x=[in_accuracy_isf,out_accuracy_isf], \n            y=['normal', 'fraud'],\n            label=\"classifiers\", \n            color=\"lightblue\", \n            ax=ax1)\nax1.set(xlim=(0,1))\n\nax2.set_title(\"Accuracy of Local Outlier Factor\",fontsize=20)\nsns.barplot(x=[in_accuracy_lof,out_accuracy_lof], \n            y=['normal', 'fraud'], \n            label=\"classifiers\", \n            color=\"lightgreen\", \n            ax=ax2)\nax2.set(xlim=(0,1))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-07T13:47:14.516073Z","iopub.execute_input":"2024-07-07T13:47:14.516524Z","iopub.status.idle":"2024-07-07T13:47:14.71809Z","shell.execute_reply.started":"2024-07-07T13:47:14.516334Z","shell.execute_reply":"2024-07-07T13:47:14.717318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CONCLUSION","metadata":{}},{"cell_type":"markdown","source":"Both, Isolation Forest and Local Outlier Factor performed same in predicting Normal cases but Isolation Forest performed far better in detecting Fraud cases.","metadata":{}}]}